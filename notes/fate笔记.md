BINARY_BCE = "binary:bce"
MULTI_CE = "multi:ce"
REGRESSION_L2 = "regression:l2"

1. **BINARY_BCE ("binary:bce")**:
   - 代表二元交叉熵损失（Binary Cross-Entropy Loss）。
   - 主要用于二分类问题，即目标变量只有两个类别（例如，0和1）。
   - 该损失函数用于衡量预测概率与实际标签之间的差异，常用于逻辑回归和神经网络的输出层。

2. **MULTI_CE ("multi:ce")**:
   - 代表多类别交叉熵损失（Multi-class Cross-Entropy Loss）。
   - 适用于多分类问题，即目标变量有多个类别。
   - 该损失函数用于评估模型预测的类别概率分布与实际类别的匹配程度，常用于softmax输出层的神经网络。

3. **REGRESSION_L2 ("regression:l2")**:
   - 代表L2损失，也称为均方误差（Mean Squared Error, MSE）。
   - 用于回归问题，即预测连续数值。
   - 该损失函数通过计算预测值与实际值之间的平方差来衡量模型的预测误差。

这些损失函数在训练机器学习模型时用于指导模型的参数更新，以最小化预测误差。选择合适的损失函数取决于具体的任务类型（分类或回归）和数据特征。


当然，这段代码定义了一个名为 `HeteroSecureBoostGuest` 的类，继承自 `HeteroBoostingTree`。这是一个用于纵向联邦学习环境下的安全增强提升（Secure Boosting）决策树模型。下面是对每个参数的详细解释：

1. **`num_trees=3`**
   - **含义**：指定提升过程中要训练的决策树的数量。更多的树通常可以提高模型的表现力，但也可能增加计算复杂度和过拟合的风险。
   
2. **`max_depth=3`**
   - **含义**：限制每棵决策树的最大深度。较浅的树有助于防止过拟合，同时提高计算效率。
   
3. **`complete_secure=0`**
   - **含义**：这是一个布尔标志，指示是否启用完全安全的计算模式。如果设置为 `1`，模型可能会使用更严格的安全协议，增加计算开销但提高数据隐私保护。
   
4. **`learning_rate=0.3`**
   - **含义**：学习率（也称为缩减率），控制每棵树对最终模型的贡献。较低的学习率通常需要更多的树来达到相同的效果，但可以提高模型的泛化能力。
   
5. **`objective="binary:bce"`**
   - **含义**：指定损失函数的类型。在这里，`"binary:bce"` 表示二元分类任务使用二元交叉熵（Binary Cross Entropy）作为损失函数。
   
6. **`num_class=1`**
   - **含义**：类别数量。对于二元分类任务，通常设置为 `1`，表示输出一个概率值。
   
7. **`max_bin=32`**
   - **含义**：特征离散化时使用的最大箱数。在基于直方图的决策树算法中，特征会被分桶以加速训练过程。
   
8. **`l2=0.1`**
   - **含义**：L2 正则化系数，用于防止模型过拟合。较高的值会增加模型的正则化强度。
   
9. **`l1=0`**
   - **含义**：L1 正则化系数，同样用于防止过拟合。L1 正则化有助于产生稀疏模型，但这里设置为 `0`，表示不使用 L1 正则化。
   
10. **`min_impurity_split=1e-2`**
    - **含义**：节点划分所需的最小纯度减少量。只有当划分能使节点的纯度减少至少 `1e-2` 时，才会进行划分。
    
11. **`min_sample_split=2`**
    - **含义**：内部节点在划分时所需的最小样本数量。设置为 `2` 意味着每个内部节点至少需要两个样本才能继续划分。
    
12. **`min_leaf_node=1`**
    - **含义**：叶子节点所需的最小样本数量。设置为 `1` 表示叶子节点可以包含至少一个样本。
    
13. **`min_child_weight=1`**
    - **含义**：子节点所需的最小权重和，通常与样本的权重（如梯度和海森矩阵）相关。这个参数有助于防止模型在噪声数据上过拟合。
    
14. **`goss=False`**
    - **含义**：是否启用梯度基的一侧采样（Gradient-based One-Side Sampling, GOSS）。GOSS 是一种用于加速梯度提升决策树训练的技术，通过选择具有较大梯度的样本来进行训练。
    
15. **`goss_start_iter=0`**
    - **含义**：指定从哪一轮迭代开始启用 GOSS。如果设置为 `0`，则从第一轮开始使用 GOSS。
    
16. **`top_rate=0.2`**
    - **含义**：在 GOSS 中，选择梯度最大的 20% 样本用于训练。这有助于模型更快地收敛。
    
17. **`other_rate=0.1`**
    - **含义**：在 GOSS 中，除了选择梯度最大的部分样本外，还选择 10% 的其他样本，以保留数据的多样性。
    
18. **`gh_pack=True`**
    - **含义**：可能指启用梯度和海森矩阵的打包传输。在联邦学习中，数据传输的效率和安全性非常重要，打包可以减少通信开销。
    
19. **`split_info_pack=True`**
    - **含义**：可能指启用分裂信息的打包传输。这有助于在联邦学习中高效、安全地传递节点分裂的信息。
    
20. **`hist_sub=True`**
    - **含义**：可能指启用直方图的子抽样或子处理。这在基于直方图的决策树算法中，可以加速训练过程并减少内存占用。
    
21. **`random_seed=42`**
    - **含义**：随机种子，用于确保模型训练过程的可重复性。相同的种子会产生相同的随机数序列，从而确保每次训练结果一致。

### 总结

`HeteroSecureBoostGuest` 类中的这些参数主要用于控制模型的复杂度、训练过程的效率以及数据隐私和安全性。在纵向联邦学习环境下，数据分布在多个参与方之间，通过这些参数的设置，可以在保证数据隐私的前提下，高效地训练出性能良好的提升决策树模型。


HeteroSecureBoostHost 类的默认参数： 
num_trees=3
max_depth=3
complete_secure=0
max_bin=32
hist_sub=True
